\chapter Статистическая теория принятия решений \label chap:3:els

\section Постановка задачи

Пусть у нас есть данные (обучающая выборка) $(x_1, y_1), \ldots, (x_n, y_n)$,
где $x_1,\ldots, x_n \in \mathbb R^d$ и $y_1, \ldots, y_n \in \mathbb R$.
Каждая пара $(x_i, y_i)$ соответствует одному объекту в обучающей выборке,
$i=1,\ldots, n$, $n$ — количество объектов, $d$ — количество известных признаков.
Наша задача: глядя на данные, в которых приведены и $x$, и $y$, научиться
предсказывать $y$ по $x$. В дальнейшем нам будут давать новые $x$ (значения
признаков для новых объектов) и нам нужно будет для них предсказать $y$.

Как отличить хороший предсказательный алгоритм от плохого? Чтобы ответить на
этот вопрос и уточнить постановку задачи, необходимо уточнить наши представления
о том, откуда взялись наши данные.

Будем обозначать $x_{data} = (x_1, \ldots, x_n)$, $y_{data}=(y_1, \ldots, y_n)$.

Мы верим в то, что существует некоторая пара случайных величин $(X, Y)$, и что наши данные являются выборкой из этой пары. 

\hide
    Точнее, мы верим в следующее. Рассмотрим последовательность случайных
    величин $(X_1, Y_1), \ldots, (X_n, Y_n)$. Будем считать, что для каждого
    $i=1, \ldots, n$, $(X_i, Y_i)$ распределены так же, как $(X, Y)$, и при этом
    для разных $i$ они независимы между собой. Точнее, они независимы в
    совокупности, то есть выполняется следующее:

    \eq 
        \mathbb P\left(\bigcap_{i=1}^n ((X_i \in A_i)\cap (Y_i \in B_i))\right)
        = \prod_{i=1}^n \mathbb P((X_i \in A_i) \cap (Y_i \in B_i))
    для любых борелевских $A_i$, $B_i$, $i=1,\ldots, n$. Это означает, что каждая
    $(X_i, Y_i)$ независима от всех остальных и любых их комбинаций.

    Мы верим в то, что наши данные $(x_1, y_1), \ldots, (x_n, y_n)$ были
    сгенерированы системой случайных величин $(X_1, Y_1), \ldots, (X_n, Y_n)$.

На менее формальном уровне это означает следующее. Пару случайных величин $(X,
Y)$ можно представлять себе как программу, которая по нашей команде генерирует
нам случайную пару $(x, y)$, где $x \in \mathbb R^d$, $y \in \mathbb R$. Мы
верим в то, что наши данные были сгенерированы путём независимых запусков такой
программы. (Каждый запуск ничего не знает обо всех остальных.)

Пусть теперь есть некоторая \emph{функция потерь} $L(y, \hat y)$, которая
показывает, насколько нам стало плохо от того, что при правильном ответе $y$
мы дали предсказание $\hat y$.

Например, популярна квадратичная функция потерь.
\eq
    L(y, \hat y)=(y-\hat y)^2
Именно её мы будем в основном рассматривать сегодня.

\question
    Предложите какую-нибудь функцию потерь, подходящую для случая, когда ошибка
    в большую сторону $(\hat y > y)$ приводит к гораздо более плачевным
    последствиям, чем ошибка в меньшую сторону.

Теперь пусть у нас есть алгоритм, предсказывающий $y$ по данному $x$, то есть
задана некоторая (обычная, детерминированная) функция $\hat y=f(x)$. Ожидаемая ошибка
предсказания (expected prediction error) — это матожидание
\equation \label eq:03:EPE
    \EPE(f)=\mathbb E[L(y, f(x)],
где $(x, y)$ распределены в соответствии с распределением $(X, Y)$. Для
фиксированного $f$ и $L$ это просто число.  Наша задача — выбрать такую функцию
$f$, которая бы минимизировала это число.

\section Регрессионная функция \label sec:3:regfun
Предположим, что мы знаем истинное распределение $(X, Y)$. На самом деле, в
реальной жизни мы его никогда не знаем, и пытаемся оценить, глядя на данные. Но
сейчас забудем про данные и будем считать, что знаем. Как тогда должно быть
устроено оптимальное предсказание?

Нам понадобится вспомогательная лемма.

\lemma \label lem:03:Y|X
    Рассмотрим случайную величину $(X, Y)$. Пусть $\phi(x, y)$ — некоторая
    функция. Тогда
    \eq
        \mathbb E_{(x, y)\sim (X, Y)}[\phi(X, Y)]=
            \mathbb E_{x \sim X}\mathbb E_{y\sim Y\mid X=x} [\phi(x, y)\mid
            X=x].
    Эту формулу следует понимать так. В левой части мы выбираем случайные величины из
    совместного распределения $(X, Y)$, для каждой вычисляем $\phi(x, y)$,
    делаем это много-много раз, затем
    усредняем то, что получилось. В правой части мы  выбираем $x$ из
    распределения $X$, затем генерируем много $y$ из условного распределения
    $Y\mid X=x$, вычислием
    $\phi(x, y)$, усредняем, записываем среднее. Потом повторяем эту операцию для
    другого случайного $x$, снова записать среднее и так много раз. Затем
    усреднить получившиеся средние. В обоих случаях получится одно и то же.
\proof
    Предположим для простоты, что существует совместная плотность $p_{X, Y}(x,
    y)$. Тогда
    \align \nonumber
        \item 
            \mathbb E_{(x, y)\sim (X, Y)}[\phi(X, Y)]&=
            \int_{\mathbb R}\int_{\mathbb R} 
                \phi(x, y) p_{X, Y}(x, y)\, dx\, dy=
        \item 
            = \int_{\mathbb R} p_{X}(x)\int_{\mathbb R} 
                \phi(x, y) \frac{p_{X, Y}(x, y)}{p_X(x)}dy&=
            \int_{\mathbb R} p_{X}(x) \int_{\mathbb R}
                \phi(x, y) p_{Y|X}(y|x)\,dy.

С помощью \ref[леммы][lem:03:Y|X], запишем \ref{eq:03:EPE} в следующем виде:
\eq
    \EPE(f)=\mathbb E_{x\sim X}\mathbb E_{y\sim Y\mid X=x}[L(y, f(x))\mid X=x].
Чтобы минимизировать эту штуку, нам нужно для каждого фиксированного $x$
минимизировать величину $\mathbb E_{y\sim Y\mid X=x}[L(y, f(x))\mid X=x]$. Для
фиксированного $x$ значение $f(x)$ — это просто число (обозначим его через
$\hat y$), то есть мы свели задачу к оптимизации функции одной переменной $\hat
y$:

\eq
    \mathbb E_{y\sim Y\mid X=x}[L(y, \hat y)\mid X=x] \to \min_{\hat y}.
Для квадратичной функции потерь $L(y, \hat y)=(y - \hat y)^2$ имеем:
\eq
    \int_{\mathbb R} (y-\hat y)^2 p_{Y|X}(y|x)\,dy\to \min_{\hat y}
Для краткости в дальнейшем будем опускать индекс у условной плотности и писать
просто $p(y|x)$.

Найдём производную по $\hat y$:
\align\nonumber
    \item 
        \frac{d}{d\hat y} \int_{\mathbb R}(y-\hat y)^2 p_{}(y|x)\, dy=
        \int_{\mathbb R}p_{}(y|x)\frac{d}{d\hat y} (y-\hat y)^2\, dy=
    \item 
        =\int_{\mathbb R}2(\hat y-y) p_{}(y|x)\, dy=
            2\int_{\mathbb R}\hat y p_{}(y|x)\, dy-2\int_{\mathbb R} y
            p_{}(y|x)\, dy=
    \item
        =2\hat y \int_{\mathbb R}p_{}(y|x)\, dy-2\mathbb E[Y\mid X=x]=
        2(\hat y-\mathbb E[Y\mid X=x]).
Приравнивая к нулю, получаем, что необходимое условие экстремума выполняется в
точке
\equation \label eq:03:reg
    \hat y = f(x) = \mathbb E[Y\mid X=x].
Легко показать (покажите!), что это действительно искомая точка минимума. Таким образом с
точки зрения квадратичной функции потерь оптимальным предсказанием для данного
$x$ является условное матожидание $Y$ при условии $X=x$.  Функция $f(x)$,
заданная уравнением \ref{eq:03:reg}, часто называется \emph{регрессионной
функцией}.

В следующий раз мы обсудим, как можно оценивать $\mathbb E[Y\mid X=x]$, не зная
распределения $(X, Y)$, но имея данные, которые являются выборкой из этого
распределения, и какие опасности нас поджидают на этом пути.
