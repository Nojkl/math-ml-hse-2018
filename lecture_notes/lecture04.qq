\chapter Шум, смещение и разброс \label chap:4:bias-variance

\section Напоминание: постановка задачи

Пусть есть пара случайных величин $(X, Y)$ и данные $\mathcal D=\\{(x_1,
y_1),\ldots,(x_n, y_n)\\}$, являющиеся выборкой из $(X, Y)$. Иными словами,
мы считаем, что каждая из пар $(x_1, y_1), \ldots, (x_n, y_n)$ распределена в
соответствии с распределением $(X, Y)$ и все пары независимы в совокупности.
(При этом $x_j$ конечно не является независимым с $y_j$.) Будем обозначать
распределение $\mathcal D$ через $(X, Y)^n$ (распределение $\mathcal D$
действительно является декартовой степенью распределения $(X, Y)$ в силу
независимости).

Пусть $x_j \in \mathbb X$, $y_j \in \mathbb Y$. Чаще всего $\mathbb X = \mathbb
R^d$, где $d$ — количество признаков. Мы также сейчас будем считать, что $\mathbb Y=\mathbb R$, то есть мы рассматриваем задачу регрессии (предсказания числовой переменной).

Алгоритм $a$ машинного обучения принимает на вход данные $\mathcal D$ и
возвращает функцию $f\colon \mathbb X \to  \mathbb Y$, которая предсказывает
значение $y$ по данному набору признаков $x$. Функция $f$ называется
\emph{обученным} алгоритмом. Формально:

\eq
    a\colon (\mathbb X, \mathbb Y)^n \to (\mathbb X \to \mathbb Y).
Для оценки качества предсказаний задана некоторая \emph{функция потерь}
\eq
    L\colon \mathbb Y\times \mathbb Y \to \mathbb R.
Величина $L(y, \hat y)$ измеряет, насколько нам плохо от того, что при
правильном ответе $y$ мы предсказали $\hat y$. Сегодня на протяжении всей лекции
мы будем рассматривать \emph{квадратичную функцию потерь}:
\eq
    L(y, \hat y)=(y - \hat y)^2.
На \ref[прошлой лекции\nonumber][chap:3:els] мы показали, что для квадратичной
функции потерь самое лучшее предсказание — это матожидание условного
распределения:
\equation \label eq:4:best
    f_{best}(x)=E[Y\mid X=x].
Проблема состоит в том, что на практике мы никогда не знаем истинное
распределение $(X, Y)$ и таким образом не можем найти матожидание
\ref{eq:4:best}. Вместо этого мы используем данные $\mathcal D$ для нахождения
некоторого приближения $f_{best}$.

\section Ожидаемая ошибка

Пусть мы получили некоторые данные $\mathcal D$, обучили на них алгоритм $a$ и
получили предсказывающую функцию $f=a(\mathcal D)$. Мы хотим минимизировать
ожидаемую ошибку предсказания на новом объекте $(x_{new}, y_{new})$, полученном
из того же распределения $(X, Y)$. Однако, теперь нам необходимо учесть, что не
только новый объект является случайной величиной, но и данные $\mathcal D$ мы
рассматриваем как случайную величину — разные обучающие выборки будут приводить
к разным функциям $f$, которые будут давать разные предсказания. Таким образом,
нас интересует следующая ожидаемая ошибка:
\equation \label eq:4:expected-loss
    \mathbb E [L(y_{new}, a(\mathcal D)(x_{new}))],
где $\mathcal D$ распределено как $(X, Y)^n$, $(x_{new}, y_{new})$ распределено как
$(X, Y)$ и независимо от $\mathcal D$. Введём обозначения:
\align
    \item f(x) & = a(\mathcal D),
    \item \hat y & = f(x_{new}).
Для фиксированного $x_{new}$, $\hat y$ — это некоторая случайная величина (т.к.
она зависит от $\mathcal D$, которая также случайная величина). Обозначим 
распределение $\hat y$ для фиксированного $x_{new}$ через $\hat Y(x_{new})$.

С помощью \ref[леммы][lem:03:Y|X] из предыдущей лекции можно переписать \ref{eq:4:expected-loss} следующим образом:
\equation \label eq:4:expected-loss-cond
    \mathbb E_{x_{new}\sim X}[\mathbb E_{y_{new}\sim Y\mid X=x_{new},\; \hat y\sim
        \hat Y(x_{new})}[L(y, \hat y)]],
где $y_{new}$ и $\hat y$ независимы. Последнее следует из того факта, что мы
считаем $(x_{new}, y_{new})$ независимым от $\mathcal D$. Иными словами, то,
какие данные мы имеем, само по себе никак не влияет на процесс генерирования
нового объекта $(x_{new}, y_{new})$ — это выглядит вполне реалистичным
предположением.

Теперь мы будем оценивать внутреннее матожидание в
\ref{eq:4:expected-loss-cond} для фиксированного $x_{new}$.

\section Разложение ожидаемой ошибки
\theorem
    Ожидаемая квадратичная ошибка представляется следующим образом:
    \equation \label eq:4:decomp
        \mathbb E [(y - \hat y)^2]=(\mathbb E [y] - \mathbb E [\hat y])^2 +
        \mathbb Dy + \mathbb D\hat y,
    где $y_{new}\sim Y\mid X=x_{new}$, $\hat y\sim \hat Y(x_{new})$,
    $y_{new}$ и $\hat y$ независимы. Первое слагаемое в сумме называется
    \emph{смещением} (bias), оно показывает систематическую ошибку алгоритма —
    отклонение усредненного предсказания от идеального предсказания 
    $\mathbb E[y]$. Второе слагаемое называется \emph{шумом} (noise), оно не
    зависит от алгоритма, а зависит только от истинного распределения $(X, Y)$.
    Шум равен ожидаемой ошибке идеального предсказывающего алгоритма. Наконец,
    третье слагаемое назыается \emph{разбросом} (variance), оно показывает,
    насколько разными могут получаться предсказания если обучать алгоритм на
    разных обучающих выборках. Иными словами, оно показывает чувствительность
    алгоритма по отношению к данным.
\proof
    Начнём с алгебраических преобразований:
    \align
        \item \nonumber
            \mathbb E[(y-\hat y)^2]&=\mathbb E[((y-\mathbb E[y]) + (\mathbb E[y] -
            \mathbb E[\hat y]) + (\mathbb E[\hat y]-\hat y))^2]=
        \item \label eq:4:proof:decomp
            &=\mathbb E[(y-\mathbb E[y])^2]+\mathbb E[(\mathbb E[y]-\mathbb E[\hat
            y])^2] + \mathbb E[(\mathbb E[\hat y]-\hat y)^2] +
        \item \label eq:4:proof:Ey
            &+2\mathbb E[(y-\mathbb E[y])(\mathbb E[y]-\mathbb E[\hat y])]+
        \item \label eq:4:proof:cov
            &+2\mathbb E[(y-\mathbb E[y])(\mathbb E[\hat y]-\hat y)]+
        \item \label eq:4:proof:Ehaty
            &+2\mathbb E[(\mathbb E[y]-\mathbb E[\hat y])
            (\mathbb E[\hat y]-\hat y)].
    Выражение $\mathbb E[y]-\mathbb E[\hat y]$ является просто числом, не
    случайной величиной, поэтому его матожидание равно ему самому и его можно
    выносить за знак матожидания. Поэтому \ref{eq:4:proof:decomp} совпадает с
    искомым разложением. (Напомним, что по определению дисперсия $\mathbb
    D[x]=\mathbb E[(x-\mathbb E[x])^2]$.) Остаётся доказать, что оставшиеся
    слагаемые нулевые.

    В \eqref{eq:4:proof:Ey} вынесем $(\mathbb E[y]-\mathbb E[\hat y])$ за
    матожидание и заметим, что $\mathbb E[y-\mathbb E[y]]=\mathbb E[y]-\mathbb
    E\mathbb E[y]=0$, т.к. матожидание матожидания равно матожиданию. Таким
    образом, слагаемое \eqref{eq:4:proof:Ey} равно нулю. Аналогично
    доказывается, что слагаемое \eqref{eq:4:proof:Ehaty} равно нулю. В слагаемом
    \ref{eq:4:proof:cov} записана (с точностью до знака) \emph{ковариация}
    случайных величин $y$ и $\hat y$. Она равна нулю при условии, что случайные
    велиины независимы.

\section Пример: метод k ближайших соседей (k-NN)
Метод k ближайших соседей (k nearest neighbors, k-NN) — простейший метод
машинного обучения. Для задачи регрессии он основан на непосредственной оценке
идеального предсказания $\mathbb E[Y\mid X=x]$ по выборке:

\equation \label eq:4:knn
    f(x)=\frac{1}{k}\sum_{j\in N_k(x)} y_j,
где $N_k(x)$ — множество индексов элементов $x_i$, являющихся $k$ ближайшими
соседями к $x$.

Для примера, рассмотрим распределение $(X, Y)$, заданное следующим образом:
\align \nonumber
    \item X \sim& \mathrm{\mathop{Uniform}}(-1, 1);
    \item \eps \sim& \mathcal N(0, \eps_0);
    \item Y=&X^2 + \eps.
Что можно сказать о смещении и разбросе для kNN при различных $k$? Рассмотрим
экстремальные случаи — $k=1$ и $k=n$. При $k=1$ предсказание равно одному из
значений $y_j$ и разброс предсказания примерно
равен разбросу условного распределения $Y \mid X=x$, то есть 
$\eps_0^2$. При $k=n$, предсказание в любой точке
есть среднее от всех $y_j$, $j=1,\ldots, n$ ($n$ — общий размер выборки).
Разброс предсказаний теперь равен $\eps_0^2/n$ (см. \ref[параграф][ssec:2:var]).

Что происходит со смещением? При $n=k$ предсказание $f(x)$ не зависит от $x$ и
его матожидание равно 
\align \nonumber
    \item \mathbb E[f(x)]=&\mathbb E\frac{y_1+\ldots + y_n}{n} =
        \mathbb E[Y]=
        \mathbb E_{X}\mathbb E[Y\mid X]=
    \item
        =&\mathbb E_{X} X^2=\frac{1}{2}\int_{-1}^1
        x^2 dx=\frac{1}{3}.
Таким образом, смещене в точках, далёких от $\sqrt{1/3}$, будет большим.

Наоборот, при $k=1$, $f(x)=y_{j}$, где $j$ таково,
что $x_{j}$ является ближайшим соседом $x$. Матожидание предсказания в этом
случае равно $x_{j}^2$, а расстояние от $x$ до $x_{j}$ составляет примерно
$2/n$ (т.к. точки $\\{x_k\\}$ распределены равномерно). Таким образом, смещение 
примерно равно $x_{j}^2 - x^2 =O(1/n)$ и становится сколь угодно маленьким при
больших $n$.

В целом, увеличение $k$ приводит к тому, что при вычислении $f(x)$ в оценку для
среднего попадает больше более далёких точек. Это приводит к уменьшению разброса
(потому что точек больше), но увеличению смещения (потому что участвуют более
далёкие точки). Это означает, что ни слишком маленькие, ни слишком большие
значения $k$ скорее всего не будут оптимальными. На практике $k$ находится путём
подбора с помощью кросс-валидации (об этом подробнее на семинаре).

Это пример так называемого bias—variance tradeoff: модель может быть либо
очень гибкой, но при этом слишком чувствительной к данным (маленькое смещение,
большой разброс), либо слишком грубой, но зато устойчивой (большое смещение,
маленький разброс).
